"""
MÃ³dulo Intent Guardrail - Aula 03
Valida a intenÃ§Ã£o do usuÃ¡rio ANTES de processar com o LLM principal

Este mÃ³dulo implementa defesa em camadas (defense in depth) para proteger
o agente de auditoria bancÃ¡ria contra:
- Prompt injection (tentativas de modificar comportamento)
- Prompt extraction (tentativas de extrair regras internas)
- Engenharia social (fingir autoridade)
- RequisiÃ§Ãµes fora de escopo

ðŸŽ¯ Objetivo PedagÃ³gico:
Demonstrar que seguranÃ§a nÃ£o Ã© um custo, Ã© um investimento. O Guardrail:
1. Protege o agente (seguranÃ§a)
2. Economiza tokens (FinOps)
3. Melhora a experiÃªncia do usuÃ¡rio (feedback imediato em caso de bloqueio)

ðŸ—ï¸ Arquitetura de Duas Camadas:
- Camada 1 (Pattern Matching): Regex compilado, custo zero, detecta ameaÃ§as Ã³bvias
- Camada 2 (LLM Classification): Gemini Flash, custo baixo, anÃ¡lise semÃ¢ntica profunda

ðŸ“Š FinOps Connection:
Se 10% das requisiÃ§Ãµes sÃ£o bloqueadas pelo guardrail, economizamos 10% dos
custos do modelo principal (Pro). O custo do Flash Ã© ~20x menor que Pro,
entÃ£o o ROI Ã© positivo a partir de 5% de bloqueios.
"""

import re
import os
import yaml
import logging
from pathlib import Path
from typing import Optional, Dict, Any, Pattern
from jinja2 import Environment, FileSystemLoader, TemplateNotFound
from pydantic import ValidationError

# Imports condicionais do Google GenAI SDK (substitui vertexai.generative_models)
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

from .models import IntentClassification, GuardrailResult
from .exceptions import IntentBlockedError, PolicyNotFoundError
from .logger import get_logger

logger = get_logger(__name__)


class IntentGuardrail:
    """
    Intent Guardrail com defesa em duas camadas.
    
    ðŸŽ¯ Aula 03 - Defensive Engineering:
    Implementa os seguintes Defensive Engineering Goals:
    - Input validation (Prompt Injection Protection)
    - System prompt protection (Privacy Enhancements)
    - Data minimization (Privacy Enhancements)
    - Audit logging (Privacy Enhancements)
    
    Arquitetura:
    - Camada 1: Pattern Matching via regex (custo zero)
    - Camada 2: LLM Classification via Flash (custo baixo)
    
    Fluxo de DecisÃ£o:
    1. Camada 1 detecta ameaÃ§a? â†’ BLOCKED (economiza tokens)
    2. Camada 1 nÃ£o detecta? â†’ Camada 2 analisa semanticamente
    3. Camada 2 nÃ£o detecta? â†’ ALLOWED (prossegue ao router)
    """
    
    def __init__(self, config_path: str = "config/intent_guardrail.yaml", client=None):
        """
        Inicializa o Intent Guardrail carregando configuraÃ§Ãµes e compilando regex.
        
        ðŸ—ï¸ Estrutura ADK:
        A configuraÃ§Ã£o estÃ¡ em config/ seguindo o padrÃ£o ADK, permitindo
        que equipes de seguranÃ§a atualizem padrÃµes sem alterar cÃ³digo Python.
        
        Args:
            config_path: Caminho para o arquivo YAML com configuraÃ§Ãµes do guardrail
            client: InstÃ¢ncia do genai.Client (opcional, necessÃ¡rio para Camada 2 em modo real)
            
        Raises:
            PolicyNotFoundError: Se o arquivo de configuraÃ§Ã£o nÃ£o existir
            ValueError: Se o YAML estiver malformado
        """
        # Resolver caminho relativo Ã  raiz do projeto
        project_root = Path(__file__).parent.parent
        self.config_path = project_root / config_path
        self.config: Dict[str, Any] = {}
        self.compiled_patterns: Dict[str, list[Pattern]] = {}
        self.client = client  # Google GenAI client para Camada 2
        
        # Carregar configuraÃ§Ãµes
        self._load_config()
        
        # Compilar padrÃµes de regex (performance)
        self._compile_patterns()
        
        # Configurar template Jinja2 para Camada 2
        self._setup_template_engine()
        
        logger.info("Intent Guardrail inicializado com sucesso")
    
    def _load_config(self) -> None:
        """
        Carrega configuraÃ§Ãµes do YAML.
        
        Raises:
            PolicyNotFoundError: Se arquivo nÃ£o existir
            ValueError: Se YAML malformado
        """
        try:
            logger.debug(f"Carregando configuraÃ§Ã£o de: {self.config_path}")
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.debug("ConfiguraÃ§Ã£o carregada com sucesso")
        except FileNotFoundError as e:
            logger.error(f"Arquivo de configuraÃ§Ã£o nÃ£o encontrado: {self.config_path}")
            raise PolicyNotFoundError(
                f"Arquivo de configuraÃ§Ã£o do guardrail nÃ£o encontrado: {self.config_path}"
            ) from e
        except yaml.YAMLError as e:
            logger.error(f"Erro ao processar YAML: {e}")
            raise ValueError(f"Erro ao processar YAML: {e}") from e
    
    def _compile_patterns(self) -> None:
        """
        Compila padrÃµes de regex para melhor performance.
        
        ðŸŽ¯ Aula 03 - Performance:
        Compilar regex uma vez no __init__() Ã© muito mais eficiente do que
        compilar a cada requisiÃ§Ã£o. Para 1000 requisiÃ§Ãµes/dia, isso economiza
        milhares de compilaÃ§Ãµes redundantes.
        """
        threat_patterns = self.config.get('threat_patterns', {})
        
        for category, patterns in threat_patterns.items():
            compiled_list = []
            for pattern in patterns:
                try:
                    # re.IGNORECASE: "Ignore" e "ignore" detectados igualmente
                    # re.UNICODE: Suporte a caracteres acentuados (portuguÃªs)
                    compiled = re.compile(pattern, re.IGNORECASE | re.UNICODE)
                    compiled_list.append(compiled)
                    logger.debug(f"PadrÃ£o compilado [{category}]: {pattern}")
                except re.error as e:
                    logger.warning(f"Regex invÃ¡lido [{category}]: {pattern} - {e}")
            
            self.compiled_patterns[category] = compiled_list
        
        logger.info(f"Compilados {sum(len(p) for p in self.compiled_patterns.values())} padrÃµes de ameaÃ§a")
    
    def _setup_template_engine(self) -> None:
        """
        Configura engine Jinja2 para renderizar template da Camada 2.
        """
        project_root = Path(__file__).parent.parent
        template_dir = project_root / "prompts"
        
        self.jinja_env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            trim_blocks=True,
            lstrip_blocks=True
        )
        logger.debug(f"Template engine configurado: {template_dir}")
    
    def validate_intent(self, user_request: str) -> GuardrailResult:
        """
        Valida a intenÃ§Ã£o do usuÃ¡rio usando as duas camadas.
        
        ðŸŽ¯ Aula 03 - Defesa em Camadas:
        Este Ã© o mÃ©todo pÃºblico principal do Guardrail. Orquestra a validaÃ§Ã£o
        em duas etapas para balancear seguranÃ§a e custo.
        
        Fluxo:
        1. Tenta Camada 1 (pattern matching - custo zero)
        2. Se Camada 1 nÃ£o detecta ameaÃ§a, usa Camada 2 (LLM - custo baixo)
        3. Retorna resultado com informaÃ§Ãµes de custo e auditoria
        
        Args:
            user_request: RequisiÃ§Ã£o do usuÃ¡rio a ser validada
            
        Returns:
            GuardrailResult com classificaÃ§Ã£o e metadados
            
        Raises:
            IntentBlockedError: Se requisiÃ§Ã£o for bloqueada (opcional, dependendo do uso)
        """
        logger.info(f"Validando intenÃ§Ã£o: {self._sanitize_for_log(user_request)}")
        
        # ------------------------------------------------------------------------
        # Camada 1: Pattern Matching (Custo Zero)
        # ------------------------------------------------------------------------
        layer1_result = self._layer1_pattern_matching(user_request)
        
        if layer1_result is not None:
            # Camada 1 detectou ameaÃ§a - bloquear imediatamente
            logger.warning(
                f"Camada 1 bloqueou requisiÃ§Ã£o: {layer1_result.intent_category} - "
                f"Riscos: {layer1_result.detected_risks}"
            )
            
            # Calcular custo evitado (nÃ£o precisou chamar LLM principal)
            cost_avoided = self._estimate_cost_avoided(user_request)
            
            return GuardrailResult(
                layer="pattern_matching",
                classification=layer1_result,
                tokens_used=0,  # Pattern matching nÃ£o usa tokens
                cost_avoided=cost_avoided
            )
        
        # ------------------------------------------------------------------------
        # Camada 2: LLM Classification (Custo Baixo)
        # ------------------------------------------------------------------------
        # Camada 1 nÃ£o detectou ameaÃ§a Ã³bvia, fazer anÃ¡lise semÃ¢ntica com Flash
        logger.debug("Camada 1 nÃ£o detectou ameaÃ§as, prosseguindo para Camada 2")
        
        layer2_result, tokens_used = self._layer2_llm_classification(user_request)
        
        # Calcular custo evitado se bloqueado pela Camada 2
        cost_avoided = 0.0
        if layer2_result.intent_category == "BLOCKED":
            cost_avoided = self._estimate_cost_avoided(user_request)
            logger.warning(
                f"Camada 2 bloqueou requisiÃ§Ã£o: {layer2_result.intent_category} - "
                f"Riscos: {layer2_result.detected_risks}"
            )
        else:
            logger.info(f"RequisiÃ§Ã£o permitida: {layer2_result.intent_category}")
        
        return GuardrailResult(
            layer="llm_classification",
            classification=layer2_result,
            tokens_used=tokens_used,
            cost_avoided=cost_avoided
        )
    
    def _layer1_pattern_matching(self, user_request: str) -> Optional[IntentClassification]:
        """
        Camada 1: Pattern Matching via regex compilado.
        
        ðŸŽ¯ Aula 03 - Custo Zero:
        Esta camada nÃ£o consome tokens nem faz chamadas a APIs. Ã‰ puro
        processamento local via regex, extremamente rÃ¡pido (<1ms).
        
        Args:
            user_request: RequisiÃ§Ã£o do usuÃ¡rio
            
        Returns:
            IntentClassification se ameaÃ§a detectada, None caso contrÃ¡rio
        """
        logger.debug("Executando Camada 1: Pattern Matching")
        
        detected_risks = []
        
        # Verificar cada categoria de ameaÃ§a
        for category, patterns in self.compiled_patterns.items():
            for pattern in patterns:
                if pattern.search(user_request):
                    detected_risks.append(category)
                    logger.debug(f"PadrÃ£o detectado [{category}]: {pattern.pattern}")
                    break  # Um match por categoria Ã© suficiente
        
        # Se nenhum padrÃ£o detectado, retornar None (passa para Camada 2)
        if not detected_risks:
            logger.debug("Camada 1: Nenhuma ameaÃ§a detectada")
            return None
        
        # AmeaÃ§a detectada - criar classificaÃ§Ã£o de bloqueio
        return IntentClassification(
            intent_category="BLOCKED",
            confidence=0.95,  # Alta confianÃ§a em pattern matching
            reasoning=(
                f"PadrÃµes de ameaÃ§a detectados na requisiÃ§Ã£o: {', '.join(detected_risks)}. "
                "RequisiÃ§Ã£o bloqueada por seguranÃ§a."
            ),
            detected_risks=detected_risks
        )
    
    def _layer2_llm_classification(self, user_request: str) -> tuple[IntentClassification, int]:
        """
        Camada 2: ClassificaÃ§Ã£o via LLM (Gemini Flash).
        
        ðŸŽ¯ Aula 03 - FinOps:
        Usa Gemini Flash (~16x mais barato que Pro) para fazer anÃ¡lise semÃ¢ntica
        profunda da intenÃ§Ã£o. O custo adicional Ã© mÃ­nimo comparado ao custo do
        modelo principal que seria executado se nÃ£o bloquearmos aqui.
        
        ðŸ“Š Exemplo de Custos (assumindo 200 tokens input + 100 tokens output):
        - Flash (classificaÃ§Ã£o): ~$0.00006
        - Pro (auditoria principal): ~$0.00125
        - Economia se bloqueado: $0.00119 (20x)
        
        Args:
            user_request: RequisiÃ§Ã£o do usuÃ¡rio
            
        Returns:
            Tupla (IntentClassification, tokens_used)
        """
        logger.debug("Executando Camada 2: LLM Classification")
        
        # Verificar se Camada 2 estÃ¡ habilitada
        llm_config = self.config.get('llm_classification', {})
        if not llm_config.get('enabled', True):
            logger.info("Camada 2 desabilitada, permitindo requisiÃ§Ã£o")
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.5,
                reasoning="Camada 2 desabilitada, requisiÃ§Ã£o permitida por padrÃ£o",
                detected_risks=[]
            ), 0
        
        # Verificar se estamos em modo USE_MOCK
        use_mock = os.getenv("USE_MOCK", "true").lower() == "true"
        
        if use_mock:
            # Modo simulaÃ§Ã£o: retorna ALLOWED sempre na Camada 2
            logger.debug("Modo simulaÃ§Ã£o: Camada 2 permite requisiÃ§Ã£o")
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.85,
                reasoning="SimulaÃ§Ã£o: RequisiÃ§Ã£o analisada semanticamente e considerada segura",
                detected_risks=[]
            ), 50  # Simula ~50 tokens de uso
        
        # Modo real: chamar Vertex AI com Flash via Google GenAI SDK
        if not GENAI_AVAILABLE or self.client is None:
            logger.warning("Google GenAI SDK nÃ£o disponÃ­vel, permitindo requisiÃ§Ã£o")
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.5,
                reasoning="Google GenAI SDK nÃ£o disponÃ­vel, requisiÃ§Ã£o permitida por fallback",
                detected_risks=[]
            ), 0
        
        try:
            # Renderizar template Jinja2
            template = self.jinja_env.get_template("intent_classifier.jinja2")
            prompt = template.render(user_request=user_request)
            
            # Configurar modelo Flash
            model_name = llm_config.get('model', 'gemini-2.5-flash')
            
            # Configurar geraÃ§Ã£o via GenerateContentConfig
            config = types.GenerateContentConfig(
                response_mime_type=llm_config.get('response_mime_type', "application/json"),
                temperature=llm_config.get('temperature', 0.1),
                max_output_tokens=llm_config.get('max_output_tokens', 256),
            )
            
            logger.debug(f"Chamando {model_name} para classificaÃ§Ã£o")
            response = self.client.models.generate_content(
                model=model_name,
                contents=prompt,
                config=config,
            )
            
            # Extrair tokens usados (candidates_token_count pode ser None no novo SDK)
            tokens_used = (
                (response.usage_metadata.prompt_token_count or 0) +
                (response.usage_metadata.candidates_token_count or 0)
            )
            
            # Validar resposta JSON com Pydantic
            classification = IntentClassification.model_validate_json(response.text)
            
            logger.info(
                f"Camada 2: {classification.intent_category} "
                f"(confidence: {classification.confidence:.2f}, tokens: {tokens_used})"
            )
            
            return classification, tokens_used
            
        except TemplateNotFound as e:
            logger.error(f"Template nÃ£o encontrado: {e}")
            # Fallback: permitir requisiÃ§Ã£o se template nÃ£o existe
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.3,
                reasoning="Template de classificaÃ§Ã£o nÃ£o encontrado, permitindo por fallback",
                detected_risks=[]
            ), 0
        except ValidationError as e:
            logger.error(f"Resposta do LLM nÃ£o vÃ¡lida: {e}")
            # Fallback: permitir requisiÃ§Ã£o se resposta invÃ¡lida
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.3,
                reasoning="Erro na validaÃ§Ã£o da resposta do classificador, permitindo por fallback",
                detected_risks=[]
            ), 0
        except Exception as e:
            logger.error(f"Erro na Camada 2: {e}", exc_info=True)
            # Fallback: permitir requisiÃ§Ã£o em caso de erro
            return IntentClassification(
                intent_category="ALLOWED",
                confidence=0.3,
                reasoning=f"Erro no classificador: {str(e)}, permitindo por fallback",
                detected_risks=[]
            ), 0
    
    def _estimate_cost_avoided(self, user_request: str) -> float:
        """
        Estima o custo que seria gasto se a requisiÃ§Ã£o bloqueada
        fosse processada pelo modelo principal (Pro).
        
        ðŸŽ¯ Aula 03 - FinOps Metrics:
        Esta mÃ©trica demonstra o valor do Guardrail em termos financeiros.
        Pode ser usada em dashboards de FinOps para justificar o investimento
        em seguranÃ§a ("o guardrail economizou X dÃ³lares este mÃªs").
        
        Args:
            user_request: RequisiÃ§Ã£o bloqueada
            
        Returns:
            Custo estimado evitado em USD
        """
        # Estimativa conservadora:
        # - Prompt template: ~500 tokens
        # - User request: ~100 tokens (mÃ©dia)
        # - Resposta: ~150 tokens (mÃ©dia)
        # Total: ~750 tokens
        
        estimated_total_tokens = 750
        
        # PreÃ§o do Pro (mais caro, que seria usado sem o guardrail)
        # Valores de referÃªncia do model_policy.yaml
        pro_input_cost_per_1k = 0.00125
        pro_output_cost_per_1k = 0.01000
        
        # Assumir 600 tokens input, 150 tokens output
        estimated_cost = (
            (600 / 1000.0) * pro_input_cost_per_1k +
            (150 / 1000.0) * pro_output_cost_per_1k
        )
        
        return round(estimated_cost, 6)
    
    def _sanitize_for_log(self, text: str, max_length: int = None) -> str:
        """
        Sanitiza texto para log aplicando data minimization.
        
        ðŸŽ¯ Aula 03 - Privacy Enhancement:
        Implementa o Defensive Engineering Goal de Data Minimization:
        - Trunca textos longos
        - Remove PII (CPF, email) se configurado
        - Protege dados sensÃ­veis em logs
        
        Args:
            text: Texto a sanitizar
            max_length: Tamanho mÃ¡ximo (default: config ou 200)
            
        Returns:
            Texto sanitizado
        """
        if max_length is None:
            data_min_config = self.config.get('data_minimization', {})
            max_length = data_min_config.get('max_log_length', 200)
        
        # Truncar se muito longo
        if len(text) > max_length:
            text = text[:max_length] + "..."
        
        # Sanitizar PII se configurado
        data_min_config = self.config.get('data_minimization', {})
        if data_min_config.get('sanitize_pii', False):
            pii_patterns = data_min_config.get('pii_patterns', {})
            
            # Sanitizar CPF
            if 'cpf' in pii_patterns:
                cpf_pattern = pii_patterns['cpf']
                text = re.sub(cpf_pattern, '***.***.***-**', text)
            
            # Sanitizar email
            if 'email' in pii_patterns:
                email_pattern = pii_patterns['email']
                text = re.sub(email_pattern, '***@***.***', text)
            
            # Sanitizar telefone
            if 'phone' in pii_patterns:
                phone_pattern = pii_patterns['phone']
                text = re.sub(phone_pattern, '(##) #####-####', text)
        
        return text
