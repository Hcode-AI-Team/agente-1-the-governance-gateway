"""
Script de Demonstra√ß√£o - Governance Gateway - Aula 01
Simula o fluxo completo de roteamento e auditoria

üéØ Objetivo da Aula 01:
Demonstrar como criar um projeto padronizado com estrutura ADK e monitorar
custos de execu√ß√£o em tempo real. Este script simula o problema real:
scripts soltos em Python tornam-se inaudit√°veis e uso indiscriminado de
modelos caros (Gemini Pro) gera desperd√≠cio financeiro invis√≠vel.

üìö Estrutura ADK (Agent Development Kit) - Aula 01:
- prompts/: Templates versionados (audit_master.jinja2)
- config/: Configura√ß√µes (model_policy.yaml, safety_settings.yaml)
- tools/: Ferramentas do agente (ser√° usado nas aulas futuras)
- src/: C√≥digo Python modular

Por que separar prompts/, tools/ e config/?
1. Versionamento: Mudan√ßas em prompts podem ser rastreadas no Git
2. Auditoria: Configura√ß√µes em YAML s√£o audit√°veis e revis√°veis
3. Reutiliza√ß√£o: Templates podem ser compartilhados entre agentes
4. Desacoplamento: Mudan√ßas n√£o requerem alterar c√≥digo Python

Fluxo de Execu√ß√£o:
1. Carrega pol√≠tica de roteamento (YAML) - seguindo padr√£o ADK
2. Para cada cen√°rio de teste:
   a. Router decide qual modelo usar (FinOps: Flash vs Pro)
   b. Simula chamada ao LLM (mock - n√£o faz chamada real)
   c. Calcula custo estimado em tempo real
   d. Exibe resultados formatados no terminal

‚ö†Ô∏è IMPORTANTE - Simula√ß√£o vs Produ√ß√£o:
Esta √© uma demonstra√ß√£o educativa. Em produ√ß√£o, substitua simulate_llm_response()
por chamadas reais ao Vertex AI usando google-cloud-aiplatform.

üîÆ Pr√≥ximas Aulas:
- Aula 02: Adicionaremos Intent Guardrail (classifica√ß√£o de inten√ß√£o segura)
- Aula 03: Integra√ß√£o real com Vertex AI e output estruturado (JSON)
"""

import json
import logging
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Tuple
from jinja2 import Template, Environment, FileSystemLoader, TemplateNotFound
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.json import JSON
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Imports do Vertex AI (condicionais - s√≥ usados se USE_MOCK=false)
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold
    VERTEXAI_AVAILABLE = True
except ImportError:
    VERTEXAI_AVAILABLE = False
    import warnings
    warnings.warn(
        "Vertex AI SDK n√£o instalado. Apenas modo simula√ß√£o dispon√≠vel. "
        "Instale com: pip install google-cloud-aiplatform>=1.74.0"
    )

from .router import ModelRouter
from .telemetry import CostEstimator
from .models import AuditResponse
from .exceptions import TemplateNotFoundError
from .logger import setup_logging, get_logger

# Configurar logging
logger = get_logger(__name__)

# Toggle para usar simula√ß√£o (mock) ou API real do Vertex AI
USE_MOCK = os.getenv("USE_MOCK", "true").lower() == "true"


def render_prompt_template(user_request: str, template_path: str = "prompts/audit_master.jinja2") -> str:
    """
    Carrega e processa o template Jinja2 do prompt de auditoria.
    
    üèóÔ∏è Estrutura ADK - Aula 01:
    Templates em prompts/ permitem:
    - Versionamento de prompts no Git
    - Reutiliza√ß√£o entre diferentes agentes
    - Mudan√ßas sem alterar c√≥digo Python
    - Auditoria de mudan√ßas em prompts
    
    üìö Engenharia de Prompt - Aula 02:
    Na pr√≥xima aula, este template ser√° expandido com:
    - Intent Guardrail (verifica√ß√£o de inten√ß√£o segura)
    - Chain-of-Thought para maior precis√£o
    - Configura√ß√£o de personas via YAML
    
    Usa Jinja2 para injetar vari√°veis dinamicamente no template.
    Isso permite versionamento de prompts e reutiliza√ß√£o.
    
    Args:
        user_request: Solicita√ß√£o do usu√°rio a ser injetada no template
        template_path: Caminho relativo para o arquivo de template
        
    Returns:
        Prompt processado com vari√°veis substitu√≠das
        
    Raises:
        FileNotFoundError: Se o template n√£o for encontrado
        TemplateError: Se houver erro no processamento do template
    """
    # Resolver caminho relativo √† raiz do projeto
    project_root = Path(__file__).parent.parent
    template_dir = project_root / "prompts"
    template_file = Path(template_path).name
    
    try:
        logger.debug(f"Renderizando template: {template_file}")
        # Configurar ambiente Jinja2 com FileSystemLoader
        env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Carregar e renderizar template
        template = env.get_template(template_file)
        rendered = template.render(user_request=user_request)
        logger.debug(f"Template renderizado com sucesso: {len(rendered)} caracteres")
        return rendered
    except TemplateNotFound as e:
        logger.error(f"Template n√£o encontrado: {template_file}")
        raise TemplateNotFoundError(
            f"Template n√£o encontrado: {template_dir / template_file}"
        ) from e
    except FileNotFoundError as e:
        logger.error(f"Diret√≥rio de templates n√£o encontrado: {template_dir}")
        raise TemplateNotFoundError(
            f"Template n√£o encontrado: {template_dir / template_file}"
        ) from e
    except Exception as e:
        logger.error(f"Erro ao processar template Jinja2: {e}", exc_info=True)
        raise ValueError(f"Erro ao processar template Jinja2: {e}") from e


def simulate_llm_response(model_name: str, user_request: str) -> Dict[str, Any]:
    """
    Simula a resposta do LLM sem fazer chamada real ao Vertex AI.
    
    ‚ö†Ô∏è IMPORTANTE - Aula 01 (Demonstra√ß√£o):
    Esta fun√ß√£o SIMULA uma resposta para focar nos conceitos de:
    - Roteamento de modelos (Router-Gateway pattern)
    - C√°lculo de custos (FinOps)
    - Estrutura ADK (separa√ß√£o de responsabilidades)
    
    üéØ Por que simula√ß√£o?
    - Evita complexidade de autentica√ß√£o ADC na primeira aula
    - Foca nos conceitos arquiteturais e FinOps
    - Permite demonstra√ß√£o sem custos reais
    
    üîÆ Aula 03 - Integra√ß√£o Real:
    Na Aula 03, substituiremos esta fun√ß√£o por:
    
    ```python
    from vertexai.preview.generative_models import GenerativeModel
    
    model = GenerativeModel(model_name)
    response = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",  # Aula 03: JSON estruturado
            "temperature": 0.1
        }
    )
    
    # Aula 03: Valida√ß√£o robusta com Pydantic
    return AuditResponse.model_validate_json(response.text)
    ```
    
    üõ°Ô∏è Aula 02 - Intent Guardrail:
    Na pr√≥xima aula, adicionaremos verifica√ß√£o de inten√ß√£o ANTES de chamar
    o modelo, bloqueando tentativas de prompt injection e engenharia social.
    
    A simula√ß√£o atual usa palavras-chave para determinar a resposta,
    simulando diferentes n√≠veis de risco e compliance.
    
    Args:
        model_name: Nome do modelo usado (ex: 'gemini-1.5-pro-001')
        user_request: Solicita√ß√£o do usu√°rio a ser analisada
        
    Returns:
        Dicion√°rio com a resposta simulada do auditor no formato:
        {
            "compliance_status": "APPROVED" | "REJECTED" | "REQUIRES_REVIEW",
            "risk_level": "LOW" | "MEDIUM" | "HIGH" | "CRITICAL",
            "audit_reasoning": "Texto explicativo"
        }
    """
    # ------------------------------------------------------------------------
    # L√≥gica de Simula√ß√£o por Palavras-chave
    # ------------------------------------------------------------------------
    # Em produ√ß√£o, esta l√≥gica seria substitu√≠da pela chamada real ao LLM
    # A simula√ß√£o usa palavras-chave para determinar o n√≠vel de risco
    request_lower = user_request.lower()
    
    # Ordem importa: verificar exclus√£o antes de outras opera√ß√µes
    if any(word in request_lower for word in ['exclus√£o', 'excluir', 'delete', 'remover', 'apagar']):
        compliance = "REJECTED"
        risk = "HIGH"
        reasoning = "Opera√ß√£o de exclus√£o de dados identificada. Rejeitada por violar pol√≠ticas de reten√ß√£o de dados."
    elif any(word in request_lower for word in ['transfer', 'transfer√™ncia', 'pix', 'pagamento']):
        compliance = "REQUIRES_REVIEW"
        risk = "MEDIUM"
        reasoning = "Opera√ß√£o financeira detectada. Requer revis√£o adicional conforme pol√≠tica de compliance."
    elif any(word in request_lower for word in ['consulta', 'saldo', 'extrato']):
        compliance = "APPROVED"
        risk = "LOW"
        reasoning = "Opera√ß√£o de consulta de baixo risco. Aprovada conforme pol√≠ticas de acesso."
    else:
        compliance = "APPROVED"
        risk = "LOW"
        reasoning = "Solicita√ß√£o gen√©rica analisada. Sem riscos identificados."
    
    # ------------------------------------------------------------------------
    # Simula√ß√£o de Diferen√ßa entre Modelos
    # ------------------------------------------------------------------------
    # Simula que o modelo Pro gera respostas mais detalhadas (mais tokens)
    # enquanto o Flash gera respostas mais concisas (menos tokens)
    # Isso afeta o c√°lculo de custos (mais tokens = maior custo)
    if 'pro' in model_name:
        # Resposta mais detalhada do Pro (simula an√°lise mais profunda)
        reasoning += " An√°lise detalhada realizada com modelo avan√ßado."
    else:
        # Resposta mais concisa do Flash (simula otimiza√ß√£o de custos)
        reasoning = reasoning[:100] + "."
    
    return {
        "compliance_status": compliance,
        "risk_level": risk,
        "audit_reasoning": reasoning
    }


def simulate_input_output(user_request: str, model_response: Dict[str, Any]) -> tuple[int, int]:
    """
    Simula o tamanho do input e output para c√°lculo de custos.
    
    üéØ Aula 01 - FinOps:
    Esta fun√ß√£o estima o tamanho do input/output para c√°lculo de custos.
    Em produ√ß√£o, estes valores viriam da API do Vertex AI que retorna
    informa√ß√µes sobre tokens usados na resposta.
    
    üìä M√©todo de Estimativa:
    1. Input: Template Jinja2 renderizado + user_request
    2. Output: JSON serializado da resposta
    
    üîÆ Aula 03 - Tokens Reais:
    Quando integrarmos com Vertex AI real, usaremos:
    ```python
    response.usage_metadata.prompt_token_count  # Input tokens
    response.usage_metadata.candidates_token_count  # Output tokens
    ```
    
    Por enquanto, simulamos calculando caracteres e convertendo para tokens
    usando tiktoken (m√©todo preciso) ou aproxima√ß√£o (fallback).
    
    Args:
        user_request: Solicita√ß√£o do usu√°rio
        model_response: Resposta do modelo (dicion√°rio)
        
    Returns:
        Tupla (input_chars, output_chars) - n√∫mero de caracteres em cada parte
    """
    # ------------------------------------------------------------------------
    # C√°lculo de Input (Prompt)
    # ------------------------------------------------------------------------
    # Simula o prompt completo que seria enviado ao modelo:
    # - Template do sistema (audit_master.jinja2) processado com Jinja2
    # - Solicita√ß√£o do usu√°rio injetada dinamicamente no template
    try:
        full_prompt = render_prompt_template(user_request)
        input_chars = len(full_prompt)
    except Exception as e:
        # Fallback: se houver erro no template, usa aproxima√ß√£o
        input_chars = len(user_request) + 500  # Aproxima√ß√£o do template
    
    # ------------------------------------------------------------------------
    # C√°lculo de Output (Resposta)
    # ------------------------------------------------------------------------
    # Simula a resposta JSON que o modelo retornaria
    # Em produ√ß√£o, este seria o texto real retornado pela API
    output_json = json.dumps(model_response, ensure_ascii=False, indent=2)
    output_chars = len(output_json)
    
    return input_chars, output_chars


def load_safety_settings() -> Dict[Any, Any]:
    """
    Carrega configura√ß√µes de seguran√ßa do arquivo YAML.
    
    üõ°Ô∏è Aula 03 - Safety Settings:
    As configura√ß√µes de seguran√ßa definem quais tipos de conte√∫do
    potencialmente prejudicial o modelo deve bloquear. Isso complementa
    o Intent Guardrail (Aula 02) que valida a pergunta do usu√°rio.
    
    Safety Settings validam a resposta do modelo:
    - Ass√©dio (HARASSMENT)
    - Discurso de √≥dio (HATE_SPEECH)
    - Conte√∫do sexual expl√≠cito (SEXUALLY_EXPLICIT)
    - Conte√∫do perigoso (DANGEROUS_CONTENT)
    
    Returns:
        Dicion√°rio mapeando HarmCategory para HarmBlockThreshold
        
    Raises:
        FileNotFoundError: Se o arquivo safety_settings.yaml n√£o existir
        ValueError: Se o YAML estiver malformado
    """
    if not VERTEXAI_AVAILABLE:
        logger.warning("Vertex AI n√£o dispon√≠vel, safety settings ignorados")
        return {}
    
    project_root = Path(__file__).parent.parent
    safety_path = project_root / "config" / "safety_settings.yaml"
    
    try:
        logger.debug(f"Carregando safety settings de: {safety_path}")
        with open(safety_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # Mapeamento de strings YAML para enums do Vertex AI
        category_map = {
            "HARM_CATEGORY_HARASSMENT": HarmCategory.HARM_CATEGORY_HARASSMENT,
            "HARM_CATEGORY_HATE_SPEECH": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            "HARM_CATEGORY_SEXUALLY_EXPLICIT": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            "HARM_CATEGORY_DANGEROUS_CONTENT": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        }
        threshold_map = {
            "BLOCK_MEDIUM_AND_ABOVE": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            "BLOCK_LOW_AND_ABOVE": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            "BLOCK_ONLY_HIGH": HarmBlockThreshold.BLOCK_ONLY_HIGH,
            "BLOCK_NONE": HarmBlockThreshold.BLOCK_NONE,
        }
        
        # Converter YAML para formato esperado pela API
        safety_settings = {
            category_map[s["category"]]: threshold_map[s["threshold"]]
            for s in data["safety_settings"]
        }
        
        logger.info(f"Safety settings carregados: {len(safety_settings)} categorias configuradas")
        return safety_settings
        
    except FileNotFoundError as e:
        logger.error(f"Arquivo safety_settings.yaml n√£o encontrado: {safety_path}")
        raise FileNotFoundError(f"Safety settings n√£o encontrado: {safety_path}") from e
    except Exception as e:
        logger.error(f"Erro ao carregar safety settings: {e}", exc_info=True)
        raise ValueError(f"Erro ao processar safety settings: {e}") from e


def call_vertex_ai(
    model_name: str, 
    prompt: str, 
    safety_settings: Dict[Any, Any] = None
) -> Tuple[Dict[str, Any], int, int]:
    """
    Faz chamada real ao Vertex AI e retorna resposta estruturada.
    
    üîó Aula 03 - Integra√ß√£o Real com Vertex AI:
    Esta fun√ß√£o substitui simulate_llm_response() quando USE_MOCK=false.
    Faz chamada real ao Gemini 2.5 Pro/Flash via Vertex AI.
    
    Diferen√ßas vs Simula√ß√£o:
    - Usa modelo real (GenerativeModel)
    - Retorna tokens REAIS (usage_metadata)
    - Gera custos reais
    - Requer autentica√ß√£o ADC
    - Valida resposta JSON com Pydantic
    
    üìä Response Estruturado:
    O par√¢metro response_mime_type="application/json" for√ßa o modelo
    a retornar JSON v√°lido, reduzindo erros de parsing e aumentando
    a confiabilidade da integra√ß√£o.
    
    Args:
        model_name: Nome do modelo Gemini (ex: 'gemini-2.5-pro')
        prompt: Prompt completo renderizado (com template Jinja2)
        safety_settings: Configura√ß√µes de seguran√ßa (opcional)
        
    Returns:
        Tupla (resposta_dict, input_tokens, output_tokens):
        - resposta_dict: Resposta do auditor validada com Pydantic
        - input_tokens: Tokens REAIS de input (do usage_metadata)
        - output_tokens: Tokens REAIS de output (do usage_metadata)
        
    Raises:
        RuntimeError: Se Vertex AI SDK n√£o estiver instalado
        ValueError: Se a resposta do modelo n√£o for JSON v√°lido
        ValidationError: Se o JSON n√£o corresponder ao schema AuditResponse
    """
    if not VERTEXAI_AVAILABLE:
        raise RuntimeError(
            "Vertex AI SDK n√£o dispon√≠vel. Instale com: "
            "pip install google-cloud-aiplatform>=1.74.0"
        )
    
    logger.info(f"Chamando Vertex AI com modelo: {model_name}")
    
    try:
        # ------------------------------------------------------------------------
        # Passo 1: Criar inst√¢ncia do modelo
        # ------------------------------------------------------------------------
        # GenerativeModel √© a classe principal do SDK do Vertex AI
        # Cada inst√¢ncia representa um modelo espec√≠fico (Pro ou Flash)
        model = GenerativeModel(model_name)
        logger.debug(f"Modelo {model_name} inicializado")
        
        # ------------------------------------------------------------------------
        # Passo 2: Configurar par√¢metros de gera√ß√£o
        # ------------------------------------------------------------------------
        # response_mime_type: For√ßa JSON estruturado (reduz erros de parsing)
        # temperature: Controla aleatoriedade (0.1 = mais determin√≠stico)
        generation_config = {
            "response_mime_type": "application/json",
            "temperature": 0.1
        }
        
        # ------------------------------------------------------------------------
        # Passo 3: Fazer chamada ao modelo
        # ------------------------------------------------------------------------
        # Esta √© a chamada real que gera custos!
        # O Vertex AI cobra por tokens de input e output
        logger.debug("Enviando requisi√ß√£o para Vertex AI...")
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        logger.debug("Resposta recebida do Vertex AI")
        
        # ------------------------------------------------------------------------
        # Passo 4: Extrair tokens REAIS da resposta
        # ------------------------------------------------------------------------
        # usage_metadata cont√©m informa√ß√µes precisas sobre tokens consumidos
        # Isso substitui a estimativa com tiktoken usada na simula√ß√£o
        input_tokens = response.usage_metadata.prompt_token_count
        output_tokens = response.usage_metadata.candidates_token_count
        logger.info(f"Tokens consumidos: input={input_tokens}, output={output_tokens}")
        
        # ------------------------------------------------------------------------
        # Passo 5: Validar JSON com Pydantic
        # ------------------------------------------------------------------------
        # model_validate_json garante que a resposta est√° no formato esperado
        # Se n√£o estiver, lan√ßa ValidationError (evita erros em produ√ß√£o)
        audit_response = AuditResponse.model_validate_json(response.text)
        logger.debug("Resposta validada com Pydantic")
        
        # Converter para dicion√°rio para compatibilidade com c√≥digo existente
        return audit_response.model_dump(), input_tokens, output_tokens
        
    except Exception as e:
        logger.error(f"Erro ao chamar Vertex AI: {e}", exc_info=True)
        raise


def main():
    """
    Fun√ß√£o principal de demonstra√ß√£o.
    
    üéØ Aula 01 - FinOps em Tempo Real:
    Simula 3 requisi√ß√µes de diferentes departamentos para demonstrar:
    - Roteamento baseado em tier (platinum, standard, budget)
    - C√°lculo de custos em tempo real
    - Compara√ß√£o Flash vs Pro
    
    üìö Conex√£o com pr√≥ximas aulas:
    - Aula 02: Cada requisi√ß√£o ser√° validada por Intent Guardrail
    - Aula 03: Substituiremos simula√ß√£o por chamadas reais ao Vertex AI
    """
    # Configurar logging para a aplica√ß√£o
    setup_logging(level="INFO")
    
    # Log do modo de opera√ß√£o
    mode_str = "SIMULA√á√ÉO (Mock)" if USE_MOCK else "PRODU√á√ÉO (Vertex AI Real)"
    logger.info(f"Iniciando Governance Gateway - Modo: {mode_str}")
    
    console = Console()
    
    # ------------------------------------------------------------------------
    # Inicializa√ß√£o do Vertex AI (apenas se USE_MOCK=false)
    # ------------------------------------------------------------------------
    # Application Default Credentials (ADC) s√£o usadas automaticamente
    # Certifique-se de executar: gcloud auth application-default login
    if not USE_MOCK:
        if not VERTEXAI_AVAILABLE:
            console.print("[bold red]ERRO: Vertex AI SDK n√£o instalado![/bold red]")
            console.print("Instale com: pip install google-cloud-aiplatform>=1.74.0")
            console.print("Ou defina USE_MOCK=true no arquivo .env para usar simula√ß√£o")
            return
        
        try:
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
            location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-east1")
            
            if not project_id:
                console.print("[bold red]ERRO: GOOGLE_CLOUD_PROJECT n√£o definido no .env[/bold red]")
                console.print("Configure o arquivo .env com seu Project ID do GCP")
                return
            
            logger.info(f"Inicializando Vertex AI: project={project_id}, location={location}")
            vertexai.init(project=project_id, location=location)
            logger.info("Vertex AI inicializado com sucesso")
            
            # Carregar safety settings
            safety_settings = load_safety_settings()
            
        except Exception as e:
            console.print(f"[bold red]Erro ao inicializar Vertex AI: {e}[/bold red]")
            console.print("Verifique:")
            console.print("1. GOOGLE_CLOUD_PROJECT est√° correto no .env")
            console.print("2. Executou: gcloud auth application-default login")
            console.print("3. Tem permiss√µes no projeto GCP")
            logger.error(f"Erro na inicializa√ß√£o: {e}", exc_info=True)
            return
    else:
        safety_settings = {}
        logger.info("Modo simula√ß√£o ativado - sem conex√£o com Vertex AI")
    
    # T√≠tulo
    console.print("\n")
    mode_badge = "[yellow]SIMULA√á√ÉO[/yellow]" if USE_MOCK else "[green]PRODU√á√ÉO[/green]"
    console.print(
        Panel.fit(
            f"[bold cyan]Governance Gateway[/bold cyan] {mode_badge}\n"
            "[dim]Sistema de Roteamento de Modelos LLM - Padr√£o Router-Gateway[/dim]",
            border_style="cyan"
        )
    )
    console.print("\n")
    
    # ------------------------------------------------------------------------
    # Inicializa√ß√£o dos Componentes
    # ------------------------------------------------------------------------
    # Router: Carrega pol√≠tica YAML e decide qual modelo usar
    # CostEstimator: Carrega pre√ßos YAML e calcula custos
    try:
        logger.info("Inicializando componentes: ModelRouter e CostEstimator")
        router = ModelRouter()
        cost_estimator = CostEstimator()
        logger.info("Componentes inicializados com sucesso")
    except Exception as e:
        logger.error(f"Erro ao inicializar componentes: {e}", exc_info=True)
        console.print(f"[bold red]Erro ao inicializar componentes: {e}[/bold red]")
        return
    
    # ------------------------------------------------------------------------
    # Cen√°rios de Teste
    # ------------------------------------------------------------------------
    # Simula requisi√ß√µes de 3 departamentos diferentes para demonstrar
    # o roteamento baseado em tier e complexidade
    scenarios = [
        {
            "department": "legal_dept",
            "department_name": "Departamento Jur√≠dico",
            "user_request": "Preciso revisar o contrato de parceria com a empresa XYZ para verificar cl√°usulas de confidencialidade",
            "complexity": 0.8
        },
        {
            "department": "hr_dept",
            "department_name": "Recursos Humanos",
            "user_request": "Verificar saldo de f√©rias do funcion√°rio ID 12345",
            "complexity": 0.3
        },
        {
            "department": "it_ops",
            "department_name": "Opera√ß√µes de TI",
            "user_request": "Consultar logs de acesso do sistema de gest√£o",
            "complexity": 0.2
        }
    ]
    
    # ------------------------------------------------------------------------
    # Processamento de Cada Cen√°rio
    # ------------------------------------------------------------------------
    for idx, scenario in enumerate(scenarios, 1):
        console.print(f"\n[bold yellow]--- Cenario {idx}: {scenario['department_name']} ---[/bold yellow]\n")
        
        # --------------------------------------------------------------------
        # Passo 1: Roteamento (Decis√£o do Modelo)
        # --------------------------------------------------------------------
        # O router consulta a pol√≠tica YAML e decide qual modelo usar
        # baseado no tier do departamento e na complexidade da requisi√ß√£o
        try:
            logger.info(f"Processando cen√°rio {idx}: {scenario['department_name']}")
            selected_model = router.route_request(
                scenario['department'],
                scenario['complexity']
            )
            logger.debug(f"Modelo selecionado: {selected_model}")
        except Exception as e:
            logger.error(f"Erro no roteamento para {scenario['department']}: {e}", exc_info=True)
            console.print(f"[bold red]Erro no roteamento: {e}[/bold red]")
            continue
        
        # --------------------------------------------------------------------
        # Passo 2: Chamada ao LLM (Mock ou Real)
        # --------------------------------------------------------------------
        # Toggle USE_MOCK determina se usa simula√ß√£o ou API real
        try:
            if USE_MOCK:
                # Modo simula√ß√£o: usa keyword matching (sem custos reais)
                logger.debug("Usando simula√ß√£o (mock)")
                response_data = simulate_llm_response(
                    selected_model,
                    scenario['user_request']
                )
                
                # Simula tamanho do input/output para c√°lculo de custos
                input_chars, output_chars = simulate_input_output(
                    scenario['user_request'],
                    response_data
                )
                
                # Calcula custo estimado (baseado em caracteres)
                estimated_cost = cost_estimator.calculate_cost(
                    selected_model,
                    input_chars,
                    output_chars
                )
                
            else:
                # Modo produ√ß√£o: usa Vertex AI real (gera custos reais)
                logger.debug("Usando Vertex AI real")
                
                # Renderizar prompt completo com template Jinja2
                prompt = render_prompt_template(scenario['user_request'])
                
                # Fazer chamada real ao Vertex AI
                response_data, input_tokens, output_tokens = call_vertex_ai(
                    selected_model,
                    prompt,
                    safety_settings
                )
                
                # Calcula custo real (baseado em tokens exatos da API)
                estimated_cost = cost_estimator.calculate_cost_from_tokens(
                    selected_model,
                    input_tokens,
                    output_tokens
                )
            
            logger.info(f"Custo calculado: ${estimated_cost:.6f} USD")
            
        except Exception as e:
            logger.error(f"Erro ao processar requisi√ß√£o: {e}", exc_info=True)
            console.print(f"[bold red]Erro ao processar requisi√ß√£o: {e}[/bold red]")
            continue
        
        # --------------------------------------------------------------------
        # Passo 3: Exibi√ß√£o de Resultados
        # --------------------------------------------------------------------
        # Usa a biblioteca Rich para criar tabelas e pain√©is formatados
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Atributo", style="cyan", width=25)
        table.add_column("Valor", style="white")
        
        table.add_row("Departamento", scenario['department_name'])
        table.add_row("Complexidade", f"{scenario['complexity']:.2f}")
        table.add_row("Modelo Escolhido", f"[bold green]{selected_model}[/bold green]")
        table.add_row("Custo Estimado", f"[bold yellow]${estimated_cost:.6f} USD[/bold yellow]")
        
        # Mostrar tokens ou chars dependendo do modo
        if USE_MOCK:
            table.add_row("Input (chars)", str(input_chars))
            table.add_row("Output (chars)", str(output_chars))
        else:
            table.add_row("Input (tokens)", str(input_tokens))
            table.add_row("Output (tokens)", str(output_tokens))
        
        console.print(table)
        
        # Exibir resposta do auditor em formato JSON formatado
        console.print("\n[bold]Resposta do Auditor:[/bold]")
        console.print(JSON(json.dumps(response_data, ensure_ascii=False, indent=2)))
        
        console.print("\n")
    
    # ------------------------------------------------------------------------
    # Resumo Final
    # ------------------------------------------------------------------------
    logger.info("Demonstra√ß√£o conclu√≠da com sucesso")
    console.print(
        Panel.fit(
            "[bold green]OK - Demonstracao concluida com sucesso![/bold green]\n"
            "[dim]O sistema demonstrou o roteamento baseado em politica YAML[/dim]",
            border_style="green"
        )
    )
    console.print("\n")


if __name__ == "__main__":
    main()
